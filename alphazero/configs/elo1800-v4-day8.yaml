# ============================================================
# V4 Day 8 - Opening Recovery 수렴 (Gradual Taper)
# Target: 공격적 탐색 → 정상 훈련으로 점진적 복귀
# Iterations: 210-240
#
# day7 (180-210) 이후 연속. 3단계 taper:
#   210-220: 약간 완화 (searches 1000, temp 1.20)
#   220-230: 중간 수렴 (searches 1400, temp 1.00)
#   230-240: 정상 복귀 (searches 1600, temp 0.85)
#
# 800 searches→1600 + 1200 games → ~1.3-2.0h/iter → 30 iters ≈ 50h (~$208)
# ============================================================

board:
  num_lines: 19
  enable_doublethree: true
  enable_capture: true
  capture_goal: 5
  gomoku_goal: 5
  history_length: 5

model:
  num_hidden: 128
  num_resblocks: 12

mcts:
  C: 2.0
  num_searches:
    - { until: 220, value: 1000 }
    - { until: 230, value: 1400 }
    - { until: 240, value: 1600 }
  exploration_turns:
    - { until: 225, value: 25 }
    - { until: 240, value: 20 }
  dirichlet_epsilon:
    - { until: 220, value: 0.45 }
    - { until: 230, value: 0.38 }
    - { until: 240, value: 0.32 }
  dirichlet_alpha:
    - { until: 220, value: 0.05 }
    - { until: 230, value: 0.08 }
    - { until: 240, value: 0.15 }
  batch_infer_size: 64
  min_batch_size: 8
  max_batch_wait_ms: 5
  use_native: true
  resign_enabled: false
  resign_threshold: 0.95
  min_moves_before_resign: 20

training:
  num_iterations: 240
  num_selfplay_iterations: 1200
  num_epochs: 2
  batch_size: 512
  learning_rate:
    - { until: 225, value: 0.0008 }
    - { until: 240, value: 0.0006 }
  weight_decay: 0.0001
  temperature:
    - { until: 220, value: 1.20 }
    - { until: 230, value: 1.00 }
    - { until: 240, value: 0.85 }
  replay_buffer_size: 350000             # 정상 buffer 크기로 복원 (250K→350K)
  min_samples_to_train: 10000
  priority_replay:
    enabled: false
    start_iteration: 15
    alpha: 0.6
    beta: 0.4
    epsilon: 0.001
  dataloader_num_workers: 4
  enable_tf32: true
  use_channels_last: true
  random_play_ratio:
    - { until: 220, value: 0.01 }
    - { until: 240, value: 0.0 }
  random_opening_turns: 0               # center 학습 완료 가정
  opponent_rates:
    random_bot_ratio: 0.06
    prev_bot_ratio: 0.25

evaluation:
  num_eval_games: 40
  eval_every_iters: 2
  promotion_win_rate: 0.55
  eval_num_searches: 1000
  elo_k_factor: 2.0
  initial_blunder_rate: 0.0
  initial_baseline_win_rate: 0.0
  blunder_increase_limit: 1.0
  baseline_wr_min: 0.0
  num_baseline_games: 16
  blunder_threshold: 0.5
  random_play_ratio: 0.0
  eval_opening_turns: 6
  eval_temperature: 1.0
  eval_dirichlet_epsilon: 0.2
  baseline_num_searches: 600
  use_sprt: false

runtime:
  selfplay:
    actor_num_cpus: 1.0
    inflight_per_actor: 192
    games_per_actor: 80
    max_chunk_size: 1
  inference:
    actor_num_gpus: 0.08
    num_actors: 12
    actor_num_cpus: 1.0
    use_local_inference: false
  evaluation:
    num_workers: 40

paths:
  use_gcs: true
  run_prefix: "gmk-testing-refactoring"
  run_id: elo1800-gcp-v4

parallel:
  ray_local_num_workers: -1

io:
  max_samples_per_shard: null
  local_replay_cache: /tmp/gmk_replay_cache
